![휴먼스케이프](https://user-images.githubusercontent.com/88444944/167796009-9b41d165-46e9-4bcc-b82e-7513e81ad8c8.jpg)


# Wanted team_B #3휴먼스케이프 기업과제  

임상정보 open API를 수집하는 WEB Aplication을 설계, 구현, 개발합니다.

## 과제 기한:
* 3 ~ 5인이 2 ~ 3일 이내로 완료하세요  
#
  1일: API분석, 모델링, 수집 스크립트(batch task) 작성

  2일: 임상정보 API 구현, 로컬 테스트 완료, API 문서화 

  3일: 배포 및 문서화, 가산점 구현(기능추가)

## Team process:

### Team 분업  ###  
  
|성명|업무|비고|
|------|---|---|
|최승리|배치스크립트 작성|팀장⭐ |
|하정현|배포(docker,swagger)|.|
|남기윤|데이터 적재 앱구현|.|\

### 중점 point

1. RESTFUL 한 API 구현 (Endpoint URL, HTTP Method , JSON Response)
2. 효율적인 쿼리 구현
3. 요구사항 뿐 아니라 다른 기능이 함게 있는 서버라고 가정하고 폴더, 파일, 코드 스트럭처를 설계

## Directory Info.

```
#최종 tree를 입력해주세요
```
## 실행 안내

**누구나 따라 할 수 있을 정도의 자세한 실행 방법, 가이드대로 실행되지 않을경우 트러블 슈팅 가이드를 함께 제시해야 합니다.**

## 휴먼스케이프 project 요구사항 분석

* 임상시험 정보를 수집하는 batch task 작성
  * open API 참고:"htps://www.data.go.kr/data/3074271/fileData.do#/API목록/GETuddi%3Acfc19dda-6f75-4c57-86a8-bb9c8b103887"
* 출제의도 :
  * open API 스펙을 보고 이해하며 데이터를 주기적으로 적재 하는 기능을 구현, 실제 데이터를 추가하면서 중복 방지에 대한 전략을 세워야함
  * 기존 데이터와 API 데이터간의 수정된 사항을 비교하여 해당 임상시험이 업데이트 된 것인지 새로 추가된 것 인지 구별이 가능해야함
  * 실행이 완료되면 추가된 건 수, 업데이트 된 건 수를 출력하거나 따로 로깅해줘야함
```
요구사항 분석
1. open API데이터를 적재하는 기능을 구현하며 중복을 방지하는 방법
  -여기에 내용을 입력해주세요
  
2.기존 데이터와 수정된 사항을 비교하여 구분하는 방법
  -여기에 내용을 입력해주세요
  
3.실행후 로깅 방법
  -여기에 내용을 입력해주세요
  
```
* 수집한 임상정보에 대한 API
  * 특정 임상정보 읽기(uuid 값은 자유)
* 수집한 임상정보 리스트 API
  * 최근 일주일 내에 업데이트 된 임상정보 리스트
  * pagination 기능
    * offset, limit로 구현
* 직접 API를 호출해서 볼 수 있는 API Document 작성
## 추가 도전과제(v2)

  * 임상시험 정보를 제공하는 다른 API를 스스로 발굴하여 batch task를 추가(가산점)
  * 배포하여 웹에서 사용 할 수 있도록 제공
    * README.md에 배포과정에 대한 가이등화 주소 제공, 설치하지 않고 확인가능한 경우 가산점
  * 임상정보 리스트 API
    * 검색기능 제공
    * pagination 기능: offset, limit구현후 새로운 방식을 제공하면 가산점

## API info.
  **request, response 둘 다 적어야합니다.
## DB info.

## 구현 과정

### 최승리
#### 임상시험 정보를 수집하는 batch task 작성
CRONTAB
1. django-crontab 사용 이유.
  * 상대적으로 구현이 간단하였고, 요구사항을 충분히 구현 가능 할 것으로 생각되었습니다.
2. 구현 방법.
  * `utils.py`에 open api로부터 데이터를 받아 올 수 있는 로직을 작성.
  * `icreat_batch.views.py`에 open api로부터 받은 데이터를 DB에 적재하고, 로깅하는 로직 작성.
3. 설정 방법 및 실행 방법.
  * `config.settings.base.py` 안에 `CRONJOBS` 부분을 수정하여 Schedule을 조정 할 수 있다.
  (현재 설정은 매주 월요일 오전 1시 실행)

  |분(0~59)|시(0~23)|일(0~31)|월(1~12)|요일(0~7)|
  |---|---|---|---|---|
  |*|*|*|*|*|

  * 요일에서 0과 1은 일요일이다.
  * 예 : (* 1 1 * *) = 매월 1일 오전 1시 실행
  * 스케줄링 설정 후 `python manage.py crontab add --settings=config.settings.local` 명령어를 사용하여 실행한다.
  * 삭제  `python manage.py crontab remove --settings=config.settings.local`
  * 조회  `python manage.py crontab show --settings=config.settings.local`
  * 실행 결과 데이터는 DB `icreat`테이블에 적재된다.
  * 에러로그는 프로젝트 root 경로내 `cron.log` 파일에 작성된다.(자동생성)
  * 실행로그는 DB `batch_log` 테이블에 적재된다.
4. 어려웠던 점.
* django_crontab도 결국 linux crontab을 작성해 주는 역할을 하는 것이지만, linux crontab을 직접 작성 하는 것보다 조금 까다로웠습니다. 저의 개발 환경은 WSL 환경이었고, 그로 인한 경로 에러와 버전 에러, 그리고 이유를 알 수 없는 무응답 에러 등이 있었습니다. 때문에 Ubuntu 22.04버전으로 듀얼부팅하여 재작성하였고 정상 작동하였습니다.

API
1. views를 사용하여 로직을 작성한 이유.
* 꼭 스케줄링을 통한 데이터 수집이 아닌, 수동으로 수집을 실행하는 경우도 있을 것으로 생각되었고, API로 간단히 실행한다면 사용성에서 더 나을것으로 생각 되었습니다.

2. 설명.
* open api로부터 전달 받은 데이터를 `Icreat.objects.create`로 데이터 생성합니다. `get_or_create`를 사용하지 않은 이유는. 만약 `unique`값인 과제 번호가 기존것과 중복된 것이 있다면 기존 데이터와 비교후 데이터가 적재되어야 하는데, `object` 형태로는 데이터 비교가 어려웠고, `queryset` 형태로 비교가 수월했기 때문입니다. 또한 추후 업데이트에서도 `queryset` 형태가 유리했습니다. 그래서 굳이 `get`을 할 필요성이 없다고 생각 되었습니다.
* 과제 번호가 중복된 것이 없다면 생성되고, 있다면 `IntegrityError` 예외처리를 통해 완전히 같은 데이터라면 `continue`, 하나의 값이라도 업데이트 된 것이 있다면, 전체 데이터를 덮어씁니다.
* 마지막으로 시작시간, 종료시간, 생성된 수, 업데이트된 수, 생성된 데이터의 과제번호 리스트, 업데이트된 데이터의 과제번호 리스트를 수집하여 `batch_log` 테이블에 로깅합니다.

3. 실행 방법.
* `METHOD = POST`, `api/v1/batch` 로 request합니다. `{'message' : "success!"}`가 출력되면 성공!
---
### 하정현
### 남기윤 (아래는 기본 양식입니다.)
  * 구현 기능 설명(캡쳐 이미지등을 활용해주세요)
  * 구현 방법과 이유 (이론적 설명을 포함해주시면 좋습니다. + ex)사용한 기술을 선정한 이유, 효율성, 확장성에대한 설명 등)
  * 구현과정에서 어려웠던 점 (100~200자 이내로 자유롭게 작성해주세요. 필수사항은 아닙니다.)
  * 구현 기능 설명2(필요 시 추가)
  * 구현 방법과 이유2
  * 구현 과정에서 어려웠던 점2
  * ...



